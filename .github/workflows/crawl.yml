name: Email Crawler Workflow

on:
  workflow_dispatch:
    inputs:
      file_name:
        description: 'Name of the file to process from Dropbox'
        required: false
        default: '500.csv'

jobs:
  email-crawler:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout repository code
      - name: Checkout code
        uses: actions/checkout@v3

      # Step 2: Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # Step 3: Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas scrapy validators dropbox

      # Step 4: Set file name
      - name: Set file name
        id: set-file-name
        run: echo "FILE_NAME=${{ github.event.inputs.file_name || '500.csv' }}" >> $GITHUB_ENV

      # Step 5: Download file from Dropbox
      - name: Download file from Dropbox
        env:
          DROPBOX_ACCESS_TOKEN: ${{ secrets.DROPBOX_ACCESS_TOKEN }}
          FILE_NAME: ${{ env.FILE_NAME }}
        run: |
          python - <<EOF
          import dropbox
          import os

          ACCESS_TOKEN = os.getenv("DROPBOX_ACCESS_TOKEN")
          FILE_NAME = os.getenv("FILE_NAME")
          if not ACCESS_TOKEN:
              raise ValueError("Dropbox Access Token is not set.")
          if not FILE_NAME:
              raise ValueError("FILE_NAME environment variable is not set.")

          dbx = dropbox.Dropbox(ACCESS_TOKEN)
          DROPBOX_FILE_PATH = f"/Masatsugu Shimizu/emailcrawlerData/{FILE_NAME}"
          LOCAL_FILE_PATH = f"./{FILE_NAME}"

          try:
              with open(LOCAL_FILE_PATH, "wb") as f:
                  metadata, res = dbx.files_download(DROPBOX_FILE_PATH)
                  f.write(res.content)
              print(f"Downloaded {DROPBOX_FILE_PATH} to {LOCAL_FILE_PATH}")
          except Exception as e:
              print(f"Error downloading file: {e}")
              raise
          EOF

      # Step 6: Debugging step to check files in the directory
      - name: List files in the working directory
        run: |
          echo "Listing all files in the working directory:"
          ls -la
          echo "Checking if the file exists:"
          [ -f "${{ env.FILE_NAME }}" ] && echo "File ${{ env.FILE_NAME }} exists" || echo "File ${{ env.FILE_NAME }} does not exist"

      # Step 7: Create a symbolic link for the input file to match the hardcoded path
      - name: Create symbolic link for input file
        run: |
          mkdir -p "/home/runner/work/C:/Users/koage/Dropbox/Masatsugu Shimizu/emailcrawlerData"
          ln -s "$(pwd)/${{ env.FILE_NAME }}" "/home/runner/work/C:/Users/koage/Dropbox/Masatsugu Shimizu/emailcrawlerData/${{ env.FILE_NAME }}"

      # Step 8: Run the Email Crawler
      - name: Run Email Crawler
        run: |
          scrapy runspider EmailToURL.py

      # Step 9: Debugging step to check the generated files
      - name: List all generated files
        run: |
          echo "Listing all files in the working directory:"
          ls -la
